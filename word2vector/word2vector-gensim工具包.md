## 1 åœ¨gensimä¸­ï¼Œword2vec ç›¸å…³çš„APIéƒ½åœ¨åŒ…gensim.models.word2vecä¸­ã€‚å’Œç®—æ³•æœ‰å…³çš„å‚æ•°éƒ½åœ¨ç±»gensim.models.word2vec.Word2Vecä¸­ã€‚ç®—æ³•éœ€è¦æ³¨æ„çš„å‚æ•°æœ‰ï¼š

1) sentences: æˆ‘ä»¬è¦åˆ†æçš„è¯­æ–™ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ–è€…ä»æ–‡ä»¶ä¸­éå†è¯»å‡ºã€‚åé¢æˆ‘ä»¬ä¼šæœ‰ä»æ–‡ä»¶è¯»å‡ºçš„ä¾‹å­ã€‚

2) size: è¯å‘é‡çš„ç»´åº¦ï¼Œé»˜è®¤å€¼æ˜¯100ã€‚è¿™ä¸ªç»´åº¦çš„å–å€¼ä¸€èˆ¬ä¸æˆ‘ä»¬çš„è¯­æ–™çš„å¤§å°ç›¸å…³ï¼Œå¦‚æœæ˜¯ä¸å¤§çš„è¯­æ–™ï¼Œæ¯”å¦‚å°äº100Mçš„æ–‡æœ¬è¯­æ–™ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼ä¸€èˆ¬å°±å¯ä»¥äº†ã€‚å¦‚æœæ˜¯è¶…å¤§çš„è¯­æ–™ï¼Œå»ºè®®å¢å¤§ç»´åº¦ã€‚

3) windowï¼šå³è¯å‘é‡ä¸Šä¸‹æ–‡æœ€å¤§è·ç¦»ï¼Œè¿™ä¸ªå‚æ•°åœ¨æˆ‘ä»¬çš„ç®—æ³•åŸç†ç¯‡ä¸­æ ‡è®°ä¸ºğ‘ï¼Œwindowè¶Šå¤§ï¼Œåˆ™å’ŒæŸä¸€è¯è¾ƒè¿œçš„è¯ä¹Ÿä¼šäº§ç”Ÿä¸Šä¸‹æ–‡å…³ç³»ã€‚é»˜è®¤å€¼ä¸º5ã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®å®é™…çš„éœ€æ±‚æ¥åŠ¨æ€è°ƒæ•´è¿™ä¸ªwindowçš„å¤§å°ã€‚å¦‚æœæ˜¯å°è¯­æ–™åˆ™è¿™ä¸ªå€¼å¯ä»¥è®¾çš„æ›´å°ã€‚å¯¹äºä¸€èˆ¬çš„è¯­æ–™è¿™ä¸ªå€¼æ¨èåœ¨[5,10]ä¹‹é—´ã€‚

4) sg: å³æˆ‘ä»¬çš„word2vecä¸¤ä¸ªæ¨¡å‹çš„é€‰æ‹©äº†ã€‚å¦‚æœæ˜¯0ï¼Œ åˆ™æ˜¯CBOWæ¨¡å‹ï¼Œæ˜¯1åˆ™æ˜¯Skip-Gramæ¨¡å‹ï¼Œé»˜è®¤æ˜¯0å³CBOWæ¨¡å‹ã€‚

5) hs: å³æˆ‘ä»¬çš„word2vecä¸¤ä¸ªè§£æ³•çš„é€‰æ‹©äº†ï¼Œå¦‚æœæ˜¯0ï¼Œ åˆ™æ˜¯Negative Samplingï¼Œæ˜¯1çš„è¯å¹¶ä¸”è´Ÿé‡‡æ ·ä¸ªæ•°negativeå¤§äº0ï¼Œ åˆ™æ˜¯Hierarchical Softmaxã€‚é»˜è®¤æ˜¯0å³Negative Samplingã€‚

6) negative:å³ä½¿ç”¨Negative Samplingæ—¶è´Ÿé‡‡æ ·çš„ä¸ªæ•°ï¼Œé»˜è®¤æ˜¯5ã€‚æ¨èåœ¨[3,10]ä¹‹é—´ã€‚è¿™ä¸ªå‚æ•°åœ¨æˆ‘ä»¬çš„ç®—æ³•åŸç†ç¯‡ä¸­æ ‡è®°ä¸ºnegã€‚

7) cbow_mean: ä»…ç”¨äºCBOWåœ¨åšæŠ•å½±çš„æ—¶å€™ï¼Œä¸º0ï¼Œåˆ™ç®—æ³•ä¸­çš„ğ‘¥ğ‘¤ä¸ºä¸Šä¸‹æ–‡çš„è¯å‘é‡ä¹‹å’Œï¼Œä¸º1åˆ™ä¸ºä¸Šä¸‹æ–‡çš„è¯å‘é‡çš„å¹³å‡å€¼ã€‚åœ¨æˆ‘ä»¬çš„åŸç†ç¯‡ä¸­ï¼Œæ˜¯æŒ‰ç…§è¯å‘é‡çš„å¹³å‡å€¼æ¥æè¿°çš„ã€‚ä¸ªäººæ¯”è¾ƒå–œæ¬¢ç”¨å¹³å‡å€¼æ¥è¡¨ç¤ºğ‘¥ğ‘¤,é»˜è®¤å€¼ä¹Ÿæ˜¯1,ä¸æ¨èä¿®æ”¹é»˜è®¤å€¼ã€‚

8) min_count:éœ€è¦è®¡ç®—è¯å‘é‡çš„æœ€å°è¯é¢‘ã€‚è¿™ä¸ªå€¼å¯ä»¥å»æ‰ä¸€äº›å¾ˆç”Ÿåƒ»çš„ä½é¢‘è¯ï¼Œé»˜è®¤æ˜¯5ã€‚å¦‚æœæ˜¯å°è¯­æ–™ï¼Œå¯ä»¥è°ƒä½è¿™ä¸ªå€¼ã€‚

9) iter: éšæœºæ¢¯åº¦ä¸‹é™æ³•ä¸­è¿­ä»£çš„æœ€å¤§æ¬¡æ•°ï¼Œé»˜è®¤æ˜¯5ã€‚å¯¹äºå¤§è¯­æ–™ï¼Œå¯ä»¥å¢å¤§è¿™ä¸ªå€¼ã€‚

10) alpha: åœ¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ä¸­è¿­ä»£çš„åˆå§‹æ­¥é•¿ã€‚ç®—æ³•åŸç†ç¯‡ä¸­æ ‡è®°ä¸ºğœ‚ï¼Œé»˜è®¤æ˜¯0.025ã€‚

11) min_alpha: ç”±äºç®—æ³•æ”¯æŒåœ¨è¿­ä»£çš„è¿‡ç¨‹ä¸­é€æ¸å‡å°æ­¥é•¿ï¼Œmin_alphaç»™å‡ºäº†æœ€å°çš„è¿­ä»£æ­¥é•¿å€¼ã€‚éšæœºæ¢¯åº¦ä¸‹é™ä¸­æ¯è½®çš„è¿­ä»£æ­¥é•¿å¯ä»¥ç”±iterï¼Œalphaï¼Œ min_alphaä¸€èµ·å¾—å‡ºã€‚è¿™éƒ¨åˆ†ç”±äºä¸æ˜¯word2vecç®—æ³•çš„æ ¸å¿ƒå†…å®¹ï¼Œå› æ­¤åœ¨åŸç†ç¯‡æˆ‘ä»¬æ²¡æœ‰æåˆ°ã€‚å¯¹äºå¤§è¯­æ–™ï¼Œéœ€è¦å¯¹alpha, min_alpha,iterä¸€èµ·è°ƒå‚ï¼Œæ¥é€‰æ‹©åˆé€‚çš„ä¸‰ä¸ªå€¼ã€‚



## 2 é¢„å¤„ç†

```python
import os
from multiprocessing import Pool
import glob
import jieba
```

```python
# åŠ è½½åœç”¨è¯è¡¨
stop_words_file = open("data/stop_words.txt", 'r',encoding="utf-8",)
stop_words = list()
for line in stop_words_file.readlines():
    line = line.strip()   # å»æ‰æ¯è¡Œæœ«å°¾çš„æ¢è¡Œç¬¦
    stop_words.append(line)
stop_words_file.close()
print(len(stop_words))
print(stop_words[300:320])
```

```python
def word_cut(file_path):
    file1, file2 = file_path
    file_w = open(file2, 'w+', encoding='utf-8')
    file_r = open(file1, 'r', encoding='utf-8')
    line = file_r.readline()
    line_count = 0
    while line:
        line_1 = line.strip()
        outstr = ''
        line_seg = jieba.cut(line_1, cut_all=False)
        for word in line_seg:  
            if word not in stop_words:
                if word != '\t':
                    outstr += word 
                    outstr += " "
        outstr = str(outstr.strip())
        if len(outstr) != 0:
            file_w.writelines(outstr+"\n")
            line_count = line_count+1
        line = file_r.readline()
        if line_count>=5000: break
    file_r.close()
    file_w.close()
    print("{} finishedï¼Œwith {} Row".format(file1, str(line_count)))
    return 
```

```python
files = [i for i in os.listdir("./data/chatpeer_random_800w") if i.endswith(".txt")]
files_r_path = [os.path.join("./data/chatpeer_random_800w", i) for i in files]
files_w_path = [os.path.join("./data/chatpeer_random_800w_cut", "cut_"+i) for i in files]
files_path = list(zip(files_r_path, files_w_path))
```

```python
from utils import parallel_apply
tmp = parallel_apply(word_cut, files_path, 12, 1000)
```

```python
# åˆ‡åˆ†å•è¯ï¼Œè¿‡æ»¤é€šç”¨è¯
# pool = Pool(6)
# b = pool.map(f, files_path)
# pool.close()
# pool.join()
```

## 3 è®­ç»ƒè¯å‘é‡

```python
from gensim.models import Word2Vec
from gensim.models import word2vec
import logging
```

```python
# è®­ç»ƒè¯å‘é‡

input_dir = "./data/chatpeer_random_800w_cut"

logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)

sentences = word2vec.PathLineSentences(input_dir)

model = Word2Vec(sentences,size=100, window=5, min_count=5,workers=16, iter=10)
```

```python
model.most_similar("å¥½è¯„", topn=10) # ä½™å¼¦è·ç¦»
```

```python
model.most_similar("å·®è¯„", topn=10)
```

```python
## ä¿å­˜è¯å‘é‡
model.wv.save_word2vec_format("model_vec.txt", fvocab=None, binary=False)
```

```python
len(model.wv.vocab.keys())
```

```python

```
